{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The great import\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/vihsd/train.csv')\n",
    "df_test = pd.read_csv('../input/vihsd/test.csv')\n",
    "df_valid = pd.read_csv('../input/vihsd/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>free_text</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Em được làm fan cứng luôn rồi nè ❤️ reaction q...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Đúng là bọn mắt híp lò xo thụt :))) bên việt n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đậu Văn Cường giờ giống thằng sida hơn à</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CÔN ĐỒ CỤC SÚC VÔ NHÂN TÍNH ĐỀ NGHI VN. NHÀ NƯ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Từ lý thuyết đến thực hành là cả 1 câu chuyện ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           free_text  label_id\n",
       "0  Em được làm fan cứng luôn rồi nè ❤️ reaction q...         0\n",
       "1  Đúng là bọn mắt híp lò xo thụt :))) bên việt n...         2\n",
       "2           Đậu Văn Cường giờ giống thằng sida hơn à         0\n",
       "3  CÔN ĐỒ CỤC SÚC VÔ NHÂN TÍNH ĐỀ NGHI VN. NHÀ NƯ...         2\n",
       "4  Từ lý thuyết đến thực hành là cả 1 câu chuyện ...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "free_text    2\n",
       "label_id     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>free_text</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20880</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      free_text  label_id\n",
       "10950       NaN         1\n",
       "20880       NaN         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['free_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just drop it \n",
    "df_train = df_train.dropna(subset=['free_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean out the emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_train['free_text'] = df_train['free_text'].apply(lambda x: re.sub(r'[^\\w\\s#@/:%.,_-]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Em được làm fan cứng luôn rồi nè  reaction quá...\n",
       "1    Đúng là bọn mắt híp lò xo thụt : bên việt nam ...\n",
       "2             Đậu Văn Cường giờ giống thằng sida hơn à\n",
       "3    CÔN ĐỒ CỤC SÚC VÔ NHÂN TÍNH ĐỀ NGHI VN. NHÀ NƯ...\n",
       "4    Từ lý thuyết đến thực hành là cả 1 câu chuyện ...\n",
       "Name: free_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['free_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the vietnamese text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all \n",
    "df_train['free_text'] = df_train['free_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate punctuation from words\n",
    "df_train['free_text'] = df_train['free_text'].apply(lambda x: re.sub(r'(?<=[^\\s])\\s*([^\\w\\s])', r' \\1', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose not to remove punctuations in this case as it may represent a sentence structure that as a whole shapes an offensive or not sentence. Thus removing punctuation may disrupt the natural structure of the text and impact downstream classifying tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>free_text</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>em được làm fan cứng luôn rồi nè  reaction quá...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>đúng là bọn mắt híp lò xo thụt : bên việt nam ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>đậu văn cường giờ giống thằng sida hơn à</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>côn đồ cục súc vô nhân tính đề nghi vn . nhà n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>từ lý thuyết đến thực hành là cả 1 câu chuyện ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           free_text  label_id\n",
       "0  em được làm fan cứng luôn rồi nè  reaction quá...         0\n",
       "1  đúng là bọn mắt híp lò xo thụt : bên việt nam ...         2\n",
       "2           đậu văn cường giờ giống thằng sida hơn à         0\n",
       "3  côn đồ cục súc vô nhân tính đề nghi vn . nhà n...         2\n",
       "4  từ lý thuyết đến thực hành là cả 1 câu chuyện ...         0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, turn bad worlds into its original form\n",
    "# form the bad words dictionaries\n",
    "bad_words_txt = '../vn_offensive_words.txt'\n",
    "bad_words_dict = {}\n",
    "with open(bad_words_txt, 'r') as f:\n",
    "    bad_words = f.read().splitlines()\n",
    "    origin = \"\"\n",
    "\n",
    "    for sent in bad_words:\n",
    "        temp = sent.split(' ')\n",
    "\n",
    "        if (len(temp) > 1 and temp[0] == '#'):\n",
    "            origin = ' '.join(temp[1:])\n",
    "            continue\n",
    "        \n",
    "        if (origin != \"\"):\n",
    "            bad_words_dict[sent] = origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buồi': 'buồi',\n",
       " 'buoi': 'buồi',\n",
       " 'dau buoi': 'buồi',\n",
       " 'daubuoi': 'buồi',\n",
       " 'caidaubuoi': 'buồi',\n",
       " 'nhucaidaubuoi': 'buồi',\n",
       " 'dau boi': 'buồi',\n",
       " 'bòi': 'buồi',\n",
       " 'dauboi': 'buồi',\n",
       " 'caidauboi': 'buồi',\n",
       " 'đầu bòy': 'buồi',\n",
       " 'đầu bùi': 'buồi',\n",
       " 'dau boy': 'buồi',\n",
       " 'dauboy': 'buồi',\n",
       " 'caidauboy': 'buồi',\n",
       " 'b`': 'buồi',\n",
       " 'cặc': 'cặc',\n",
       " 'cak': 'cặc',\n",
       " 'kak': 'cặc',\n",
       " 'kac': 'cặc',\n",
       " 'cac': 'cặc',\n",
       " 'concak': 'cặc',\n",
       " 'nungcak': 'cặc',\n",
       " 'bucak': 'cặc',\n",
       " 'caiconcac': 'cặc',\n",
       " 'caiconcak': 'cặc',\n",
       " 'cu': 'cặc',\n",
       " 'cặk': 'cặc',\n",
       " 'dái': 'cặc',\n",
       " 'giái': 'cặc',\n",
       " 'zái': 'cặc',\n",
       " 'kiu': 'cặc',\n",
       " 'cứt': 'cứt',\n",
       " 'cuccut': 'cứt',\n",
       " 'cutcut': 'cứt',\n",
       " 'cứk': 'cứt',\n",
       " 'cuk': 'cứt',\n",
       " 'cười ỉa': 'cứt',\n",
       " 'cười ẻ': 'cứt',\n",
       " 'đéo': 'đéo',\n",
       " 'đếch': 'đéo',\n",
       " 'đếk': 'đéo',\n",
       " 'dek': 'đéo',\n",
       " 'đết': 'đéo',\n",
       " 'đệt': 'địt',\n",
       " 'đách': 'đéo',\n",
       " 'dech': 'đéo',\n",
       " \"đ'\": 'đéo',\n",
       " 'deo': 'đéo',\n",
       " \"d'\": 'đéo',\n",
       " 'đel': 'đéo',\n",
       " 'đél': 'đéo',\n",
       " 'del': 'đéo',\n",
       " 'dell ngửi': 'đéo',\n",
       " 'dell ngui': 'đéo',\n",
       " 'dell chịu': 'đéo',\n",
       " 'dell chiu': 'đéo',\n",
       " 'dell hiểu': 'đéo',\n",
       " 'dell hieu': 'đéo',\n",
       " 'dellhieukieugi': 'đéo',\n",
       " 'dell nói': 'đéo',\n",
       " 'dell noi': 'đéo',\n",
       " 'dellnoinhieu': 'đéo',\n",
       " 'dell biết': 'đéo',\n",
       " 'dell biet': 'đéo',\n",
       " 'dell nghe': 'đéo',\n",
       " 'dell ăn': 'đéo',\n",
       " 'dell an': 'đéo',\n",
       " 'dell được': 'đéo',\n",
       " 'dell duoc': 'đéo',\n",
       " 'dell làm': 'đéo',\n",
       " 'dell lam': 'đéo',\n",
       " 'dell đi': 'đéo',\n",
       " 'dell di': 'đéo',\n",
       " 'dell chạy': 'đéo',\n",
       " 'dell chay': 'đéo',\n",
       " 'deohieukieugi': 'đéo',\n",
       " 'địt': 'địt',\n",
       " 'đm': 'địt',\n",
       " 'dm': 'địt',\n",
       " 'đmm': 'địt',\n",
       " 'dmm': 'địt',\n",
       " 'đmmm': 'địt',\n",
       " 'dmmm': 'địt',\n",
       " 'đmmmm': 'địt',\n",
       " 'dmmmm': 'địt',\n",
       " 'đmmmmm': 'địt',\n",
       " 'dmmmmm': 'địt',\n",
       " 'đcm': 'địt',\n",
       " 'dcm': 'địt',\n",
       " 'đcmm': 'địt',\n",
       " 'dcmm': 'địt',\n",
       " 'đcmmm': 'địt',\n",
       " 'dcmmm': 'địt',\n",
       " 'đcmmmm': 'địt',\n",
       " 'dcmmmm': 'địt',\n",
       " 'đệch': 'địt',\n",
       " 'dit': 'địt',\n",
       " 'dis': 'địt',\n",
       " 'diz': 'địt',\n",
       " 'đjt': 'địt',\n",
       " 'djt': 'địt',\n",
       " 'địt mẹ': 'địt',\n",
       " 'địt mịe': 'địt',\n",
       " 'địt má': 'địt',\n",
       " 'địt mía': 'địt',\n",
       " 'địt ba': 'địt',\n",
       " 'địt bà': 'địt',\n",
       " 'địt cha': 'địt',\n",
       " 'địt con': 'địt',\n",
       " 'địt bố': 'địt',\n",
       " 'địt cụ': 'địt',\n",
       " 'dis me': 'địt',\n",
       " 'disme': 'địt',\n",
       " 'dismje': 'địt',\n",
       " 'dismia': 'địt',\n",
       " 'dis mia': 'địt',\n",
       " 'dis mie': 'địt',\n",
       " 'đis mịa': 'địt',\n",
       " 'đis mịe': 'địt',\n",
       " 'ditmemayconcho': 'địt',\n",
       " 'ditmemay': 'địt',\n",
       " 'ditmethangoccho': 'địt',\n",
       " 'ditmeconcho': 'địt',\n",
       " 'dmconcho': 'địt',\n",
       " 'dmcs': 'địt',\n",
       " 'ditmecondi': 'địt',\n",
       " 'ditmecondicho': 'địt',\n",
       " 'đụ': 'dit',\n",
       " 'đụ mẹ': 'đù má',\n",
       " 'đụ mịa': 'đù má',\n",
       " 'đụ mịe': 'đù má',\n",
       " 'đụ má': 'đù má',\n",
       " 'đụ cha': 'đù má',\n",
       " 'đụ bà': 'đù má',\n",
       " 'đú cha': 'đù má',\n",
       " 'đú con mẹ': 'đù má',\n",
       " 'đú má': 'đù má',\n",
       " 'đú mẹ': 'đù má',\n",
       " 'đù cha': 'đù má',\n",
       " 'đù má': 'đù má',\n",
       " 'đù mẹ': 'đù má',\n",
       " 'đù mịe': 'đù má',\n",
       " 'đù mịa': 'đù má',\n",
       " 'đủ cha': 'đù má',\n",
       " 'đủ má': 'đù má',\n",
       " 'đủ mẹ': 'đù má',\n",
       " 'đủ mé': 'đù má',\n",
       " 'đủ mía': 'đù má',\n",
       " 'đủ mịa': 'đù má',\n",
       " 'đủ mịe': 'đù má',\n",
       " 'đủ mie': 'đù má',\n",
       " 'đủ mia': 'đù má',\n",
       " 'đìu': 'đù má',\n",
       " 'đờ mờ': 'đù má',\n",
       " 'đê mờ': 'đù má',\n",
       " 'đờ ma ma': 'đù má',\n",
       " 'đờ mama': 'đù má',\n",
       " 'đê mama': 'đù má',\n",
       " 'đề mama': 'đù má',\n",
       " 'đê ma ma': 'đù má',\n",
       " 'đề ma ma': 'đù má',\n",
       " 'dou': 'đù má',\n",
       " 'doma': 'đù má',\n",
       " 'duoma': 'đù má',\n",
       " 'dou má': 'đù má',\n",
       " 'duo má': 'đù má',\n",
       " 'dou ma': 'đù má',\n",
       " 'đou má': 'đù má',\n",
       " 'đìu má': 'đù má',\n",
       " 'á đù': 'đù má',\n",
       " 'á đìu': 'đù má',\n",
       " 'đậu mẹ': 'đù má',\n",
       " 'đậu má': 'đù má',\n",
       " 'đĩ': 'đĩ',\n",
       " 'di~': 'đĩ',\n",
       " 'đuỹ': 'đĩ',\n",
       " 'điếm': 'đĩ',\n",
       " 'cđĩ': 'đĩ',\n",
       " 'cdi~': 'đĩ',\n",
       " 'đilol': 'đĩ',\n",
       " 'điloz': 'đĩ',\n",
       " 'đilon': 'đĩ',\n",
       " 'diloz': 'đĩ',\n",
       " 'dilol': 'đĩ',\n",
       " 'dilon': 'đĩ',\n",
       " 'condi': 'đĩ',\n",
       " 'condi~': 'đĩ',\n",
       " 'dime': 'đĩ',\n",
       " 'di me': 'đĩ',\n",
       " 'dimemay': 'đĩ',\n",
       " 'condime': 'đĩ',\n",
       " 'condimay': 'đĩ',\n",
       " 'condimemay': 'đĩ',\n",
       " 'con di cho': 'đĩ',\n",
       " \"con di cho'\": 'đĩ',\n",
       " 'condicho': 'đĩ',\n",
       " 'bitch': 'đĩ',\n",
       " 'biz': 'đĩ',\n",
       " 'bít chi': 'đĩ',\n",
       " 'con bích': 'đĩ',\n",
       " 'con bic': 'đĩ',\n",
       " 'con bíc': 'đĩ',\n",
       " 'con bít': 'đĩ',\n",
       " 'phò': 'đĩ',\n",
       " '4`': 'đĩ',\n",
       " 'lồn': 'lồn',\n",
       " 'l`': 'lồn',\n",
       " 'loz': 'lồn',\n",
       " 'lìn': 'lồn',\n",
       " 'nulo': 'lồn',\n",
       " 'ml': 'lồn',\n",
       " 'matlon': 'lồn',\n",
       " 'cailon': 'lồn',\n",
       " 'matlol': 'lồn',\n",
       " 'matloz': 'lồn',\n",
       " 'thml': 'lồn',\n",
       " 'thangmatlon': 'lồn',\n",
       " 'thangml': 'lồn',\n",
       " 'đỗn lì': 'lồn',\n",
       " 'tml': 'lồn',\n",
       " 'diml': 'lồn',\n",
       " 'dml': 'lồn',\n",
       " 'hãm': 'lồn',\n",
       " 'xàm lol': 'lồn',\n",
       " 'xam lol': 'lồn',\n",
       " 'xạo lol': 'lồn',\n",
       " 'xao lol': 'lồn',\n",
       " 'con lol': 'lồn',\n",
       " 'ăn lol': 'lồn',\n",
       " 'an lol': 'lồn',\n",
       " 'mát lol': 'lồn',\n",
       " 'mat lol': 'lồn',\n",
       " 'cái lol': 'lồn',\n",
       " 'cai lol': 'lồn',\n",
       " 'lòi lol': 'lồn',\n",
       " 'loi lol': 'lồn',\n",
       " 'ham lol': 'lồn',\n",
       " 'củ lol': 'lồn',\n",
       " 'cu lol': 'lồn',\n",
       " 'ngu lol': 'lồn',\n",
       " 'tuổi lol': 'lồn',\n",
       " 'tuoi lol': 'lồn',\n",
       " 'mõm lol': 'lồn',\n",
       " 'mồm lol': 'lồn',\n",
       " 'mom lol': 'lồn',\n",
       " 'như lol': 'lồn',\n",
       " 'nhu lol': 'lồn',\n",
       " 'nứng lol': 'lồn',\n",
       " 'nung lol': 'lồn',\n",
       " 'nug lol': 'lồn',\n",
       " 'nuglol': 'lồn',\n",
       " 'rảnh lol': 'lồn',\n",
       " 'ranh lol': 'lồn',\n",
       " 'đách lol': 'lồn',\n",
       " 'dach lol': 'lồn',\n",
       " 'mu lol': 'lồn',\n",
       " 'banh lol': 'lồn',\n",
       " 'tét lol': 'lồn',\n",
       " 'tet lol': 'lồn',\n",
       " 'vạch lol': 'lồn',\n",
       " 'vach lol': 'lồn',\n",
       " 'cào lol': 'lồn',\n",
       " 'cao lol': 'lồn',\n",
       " 'tung lol': 'lồn',\n",
       " 'mặt lol': 'lồn',\n",
       " 'xàm lon': 'lồn',\n",
       " 'xam lon': 'lồn',\n",
       " 'xạo lon': 'lồn',\n",
       " 'xao lon': 'lồn',\n",
       " 'con lon': 'lồn',\n",
       " 'ăn lon': 'lồn',\n",
       " 'an lon': 'lồn',\n",
       " 'mát lon': 'lồn',\n",
       " 'mat lon': 'lồn',\n",
       " 'cái lon': 'lồn',\n",
       " 'cai lon': 'lồn',\n",
       " 'lòi lon': 'lồn',\n",
       " 'loi lon': 'lồn',\n",
       " 'ham lon': 'lồn',\n",
       " 'củ lon': 'lồn',\n",
       " 'cu lon': 'lồn',\n",
       " 'ngu lon': 'lồn',\n",
       " 'tuổi lon': 'lồn',\n",
       " 'tuoi lon': 'lồn',\n",
       " 'mõm lon': 'lồn',\n",
       " 'mồm lon': 'lồn',\n",
       " 'mom lon': 'lồn',\n",
       " 'như lon': 'lồn',\n",
       " 'nhu lon': 'lồn',\n",
       " 'nứng lon': 'lồn',\n",
       " 'nung lon': 'lồn',\n",
       " 'nug lon': 'lồn',\n",
       " 'nuglon': 'lồn',\n",
       " 'rảnh lon': 'lồn',\n",
       " 'ranh lon': 'lồn',\n",
       " 'đách lon': 'lồn',\n",
       " 'dach lon': 'lồn',\n",
       " 'mu lon': 'lồn',\n",
       " 'banh lon': 'lồn',\n",
       " 'tét lon': 'lồn',\n",
       " 'tet lon': 'lồn',\n",
       " 'vạch lon': 'lồn',\n",
       " 'vach lon': 'lồn',\n",
       " 'cào lon': 'lồn',\n",
       " 'cao lon': 'lồn',\n",
       " 'tung lon': 'lồn',\n",
       " 'mặt lon': 'lồn',\n",
       " 'cái lờ': 'lồn',\n",
       " 'cl': 'lồn',\n",
       " 'clgt': 'lồn',\n",
       " 'cờ lờ gờ tờ': 'lồn',\n",
       " 'cái lề gì thốn': 'lồn',\n",
       " 'đốn cửa lòng': 'lồn',\n",
       " 'sml': 'lồn',\n",
       " 'sapmatlol': 'lồn',\n",
       " 'sapmatlon': 'lồn',\n",
       " 'sapmatloz': 'lồn',\n",
       " 'sấp mặt': 'lồn',\n",
       " 'sap mat': 'lồn',\n",
       " 'vlon': 'lồn',\n",
       " 'vloz': 'lồn',\n",
       " 'vlol': 'lồn',\n",
       " 'vailon': 'lồn',\n",
       " 'vai lon': 'lồn',\n",
       " 'vai lol': 'lồn',\n",
       " 'vailol': 'lồn',\n",
       " 'nốn lừng': 'lồn',\n",
       " 'vcl': 'lồn',\n",
       " 'vl': 'lồn',\n",
       " 'vleu': 'lồn',\n",
       " 'chịch': 'dit',\n",
       " 'chich': 'dit',\n",
       " 'vãi': 'dit',\n",
       " 'v~': 'dit',\n",
       " 'nứng': 'dit',\n",
       " 'nug': 'dit',\n",
       " 'đút đít': 'dit',\n",
       " 'chổng mông': 'dit',\n",
       " 'banh háng': 'dit',\n",
       " 'xéo háng': 'dit',\n",
       " 'xhct': 'dit',\n",
       " 'xephinh': 'dit',\n",
       " 'la liếm': 'dit',\n",
       " 'đổ vỏ': 'dit',\n",
       " 'xoạc': 'dit',\n",
       " 'xoac': 'dit',\n",
       " 'chich choac': 'dit',\n",
       " 'húp sò': 'dit',\n",
       " 'fuck': 'dit',\n",
       " 'fck': 'dit',\n",
       " 'bỏ bú': 'dit',\n",
       " 'buscu': 'dit',\n",
       " 'ngu': 'ngu',\n",
       " 'óc chó': 'ngu',\n",
       " 'occho': 'ngu',\n",
       " 'lao cho': 'ngu',\n",
       " 'láo chó': 'ngu',\n",
       " 'bố láo': 'ngu',\n",
       " 'chó má': 'ngu',\n",
       " 'cờ hó': 'ngu',\n",
       " 'sảng': 'ngu',\n",
       " 'thằng chó': 'ngu',\n",
       " \"thang cho'\": 'ngu',\n",
       " 'thang cho': 'ngu',\n",
       " 'chó điên': 'ngu',\n",
       " 'thằng điên': 'ngu',\n",
       " 'thang dien': 'ngu',\n",
       " 'đồ điên': 'ngu',\n",
       " 'sủa bậy': 'ngu',\n",
       " 'sủa tiếp': 'ngu',\n",
       " 'sủa đi': 'ngu',\n",
       " 'sủa càn': 'ngu',\n",
       " 'mẹ bà': 'mẹ cha',\n",
       " 'mẹ cha mày': 'mẹ cha',\n",
       " 'me cha may': 'mẹ cha',\n",
       " 'mẹ cha anh': 'mẹ cha',\n",
       " 'mẹ cha nhà anh': 'mẹ cha',\n",
       " 'mẹ cha nhà mày': 'mẹ cha',\n",
       " 'me cha nha may': 'mẹ cha',\n",
       " 'mả cha mày': 'mẹ cha',\n",
       " 'mả cha nhà mày': 'mẹ cha',\n",
       " 'ma cha may': 'mẹ cha',\n",
       " 'ma cha nha may': 'mẹ cha',\n",
       " 'mả mẹ': 'mẹ cha',\n",
       " 'mả cha': 'mẹ cha',\n",
       " 'kệ mẹ': 'mẹ cha',\n",
       " 'kệ mịe': 'mẹ cha',\n",
       " 'kệ mịa': 'mẹ cha',\n",
       " 'kệ mje': 'mẹ cha',\n",
       " 'kệ mja': 'mẹ cha',\n",
       " 'ke me': 'mẹ cha',\n",
       " 'ke mie': 'mẹ cha',\n",
       " 'ke mia': 'mẹ cha',\n",
       " 'ke mja': 'mẹ cha',\n",
       " 'ke mje': 'mẹ cha',\n",
       " 'bỏ mẹ': 'mẹ cha',\n",
       " 'bỏ mịa': 'mẹ cha',\n",
       " 'bỏ mịe': 'mẹ cha',\n",
       " 'bỏ mja': 'mẹ cha',\n",
       " 'bỏ mje': 'mẹ cha',\n",
       " 'bo me': 'mẹ cha',\n",
       " 'bo mia': 'mẹ cha',\n",
       " 'bo mie': 'mẹ cha',\n",
       " 'bo mje': 'mẹ cha',\n",
       " 'bo mja': 'mẹ cha',\n",
       " 'chetme': 'mẹ cha',\n",
       " 'chet me': 'mẹ cha',\n",
       " 'chết mẹ': 'mẹ cha',\n",
       " 'chết mịa': 'mẹ cha',\n",
       " 'chết mja': 'mẹ cha',\n",
       " 'chết mịe': 'mẹ cha',\n",
       " 'chết mie': 'mẹ cha',\n",
       " 'chet mia': 'mẹ cha',\n",
       " 'chet mie': 'mẹ cha',\n",
       " 'chet mja': 'mẹ cha',\n",
       " 'chet mje': 'mẹ cha',\n",
       " 'thấy mẹ': 'mẹ cha',\n",
       " 'thấy mịe': 'mẹ cha',\n",
       " 'thấy mịa': 'mẹ cha',\n",
       " 'thay me': 'mẹ cha',\n",
       " 'thay mie': 'mẹ cha',\n",
       " 'thay mia': 'mẹ cha',\n",
       " 'tổ cha': 'mẹ cha',\n",
       " 'bà cha mày': 'mẹ cha',\n",
       " 'cmn': 'mẹ cha',\n",
       " 'cmnl': 'mẹ cha',\n",
       " 'tiên sư nhà mày': 'tiên sư',\n",
       " 'tiên sư bố': 'tiên sư',\n",
       " 'tổ sư': 'tiên sư',\n",
       " '': 'tiên sư'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorry for the bad words :(\n",
    "bad_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all bad words variants with its original form\n",
    "def replace_bad_words(text):\n",
    "    for bad, origin in bad_words_dict.items():\n",
    "        text = text.replace(bad, origin)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function might be useful later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check output distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "0    19885\n",
       "2     2556\n",
       "1     1605\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following label:\n",
    "*   0: non-offensive\n",
    "*   1: Offensive\n",
    "*   2: Hate \n",
    "\n",
    "We see here the data is imbalance.\n",
    "1 and 2 are similar, differ only at its level of hate. Thus as 0s outnumber the other 2 labels, we shall merge 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "0    19885\n",
       "1     4161\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label_id'] = df_train['label_id'].apply(lambda x: 1 if x in [1, 2] else x)\n",
    "df_train['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yet the data is still imbalance. We shall counter it with the choice of metrics later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the same processing step for test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na\n",
    "df_test = df_test.dropna(subset=['free_text'])\n",
    "df_valid = df_valid.dropna(subset=['free_text'])\n",
    "\n",
    "# clean the emoji \n",
    "df_valid['free_text'] = df_valid['free_text'].apply(lambda x: re.sub(r'[^\\w\\s#@/:%.,_-]', '', x))\n",
    "df_test['free_text'] = df_test['free_text'].apply(lambda x: re.sub(r'[^\\w\\s#@/:%.,_-]', '', x))\n",
    "\n",
    "# standardize the text\n",
    "# lowercase all\n",
    "df_valid['free_text'] = df_valid['free_text'].apply(lambda x: x.lower())\n",
    "df_test['free_text'] = df_test['free_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# separate punctuation from words\n",
    "df_valid['free_text'] = df_valid['free_text'].apply(lambda x: re.sub(r'(?<=[^\\s])\\s*([^\\w\\s])', r' \\1', x))\n",
    "df_test['free_text'] = df_test['free_text'].apply(lambda x: re.sub(r'(?<=[^\\s])\\s*([^\\w\\s])', r' \\1', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 1 and 2 labels\n",
    "df_valid['label_id'] = df_valid['label_id'].apply(lambda x: 1 if x in [1, 2] else x)\n",
    "df_test['label_id'] = df_test['label_id'].apply(lambda x: 1 if x in [1, 2] else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_id\n",
       "0    2190\n",
       "1     482\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid['label_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark : Bag-of-words with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this basic model as a simple benchmark for out task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kfold\n",
       "0    4810\n",
       "1    4809\n",
       "2    4809\n",
       "3    4809\n",
       "4    4809\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply k fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "df_train['kfold'] = -1\n",
    "\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "y = df_train['label_id'].values\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df_train, y=y)):\n",
    "    df_train.loc[v_, 'kfold'] = f\n",
    "\n",
    "df_train['kfold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "0.8401247401247401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      3977\n",
      "           1       0.66      0.16      0.25       833\n",
      "\n",
      "    accuracy                           0.84      4810\n",
      "   macro avg       0.76      0.57      0.58      4810\n",
      "weighted avg       0.82      0.84      0.80      4810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "0.8380120607194843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      3977\n",
      "           1       0.63      0.15      0.25       832\n",
      "\n",
      "    accuracy                           0.84      4809\n",
      "   macro avg       0.74      0.57      0.58      4809\n",
      "weighted avg       0.81      0.84      0.79      4809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "0.8342690788105636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      3977\n",
      "           1       0.57      0.17      0.26       832\n",
      "\n",
      "    accuracy                           0.83      4809\n",
      "   macro avg       0.71      0.57      0.58      4809\n",
      "weighted avg       0.80      0.83      0.79      4809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n",
      "0.834477022249948\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      3977\n",
      "           1       0.58      0.15      0.24       832\n",
      "\n",
      "    accuracy                           0.83      4809\n",
      "   macro avg       0.71      0.56      0.57      4809\n",
      "weighted avg       0.80      0.83      0.79      4809\n",
      "\n",
      "Fold 4\n",
      "0.8348929091287169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91      3977\n",
      "           1       0.58      0.16      0.25       832\n",
      "\n",
      "    accuracy                           0.83      4809\n",
      "   macro avg       0.72      0.57      0.58      4809\n",
      "weighted avg       0.80      0.83      0.79      4809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "for fold_ in range(5):\n",
    "    train_df = df_train[df_train.kfold != fold_]\n",
    "    valid_df = df_train[df_train.kfold == fold_]\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=ViTokenizer.tokenize)\n",
    "    vectorizer.fit(train_df['free_text'])\n",
    "    \n",
    "    x_train = vectorizer.transform(train_df['free_text'])\n",
    "    x_valid = vectorizer.transform(valid_df['free_text'])\n",
    "\n",
    "    y_train = train_df['label_id']\n",
    "    y_valid = valid_df['label_id']\n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # threshold currently 0.5\n",
    "    preds = model.predict(x_valid)\n",
    "    print(f'Fold {fold_}')\n",
    "    print(accuracy_score(y_valid, preds))\n",
    "    print(classification_report(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say in this problem, I think we focus on recall better, as the cost of missing some hateful comments is high (may affect children). Our recall is already very high.\n",
    "\n",
    "Average accuracy : ~0.84\n",
    "\n",
    "Average F1 : ~0.91\n",
    "\n",
    "Average precision : ~0.85\n",
    "\n",
    "Average Recall : ~0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use n-gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually bad word phrases in Vietnamese goes in pair or group of 3, I have an intuition that using n-gram with bag-of-words can be useful. \n",
    "\n",
    "It is also easy to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "0.8866943866943867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93      3977\n",
      "           1       0.73      0.55      0.63       833\n",
      "\n",
      "    accuracy                           0.89      4810\n",
      "   macro avg       0.82      0.75      0.78      4810\n",
      "weighted avg       0.88      0.89      0.88      4810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "0.8956123934289874\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      3977\n",
      "           1       0.75      0.59      0.66       832\n",
      "\n",
      "    accuracy                           0.90      4809\n",
      "   macro avg       0.83      0.78      0.80      4809\n",
      "weighted avg       0.89      0.90      0.89      4809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "0.8837596173840715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93      3977\n",
      "           1       0.72      0.54      0.62       832\n",
      "\n",
      "    accuracy                           0.88      4809\n",
      "   macro avg       0.81      0.75      0.77      4809\n",
      "weighted avg       0.88      0.88      0.88      4809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n",
      "0.8808484092326887\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      3977\n",
      "           1       0.70      0.55      0.61       832\n",
      "\n",
      "    accuracy                           0.88      4809\n",
      "   macro avg       0.80      0.75      0.77      4809\n",
      "weighted avg       0.87      0.88      0.88      4809\n",
      "\n",
      "Fold 4\n",
      "0.8862549386566854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      3977\n",
      "           1       0.71      0.57      0.63       832\n",
      "\n",
      "    accuracy                           0.89      4809\n",
      "   macro avg       0.81      0.76      0.78      4809\n",
      "weighted avg       0.88      0.89      0.88      4809\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "for fold_ in range(5):\n",
    "    train_df = df_train[df_train.kfold != fold_]\n",
    "    valid_df = df_train[df_train.kfold == fold_]\n",
    "\n",
    "    vectorizer = CountVectorizer(tokenizer=ViTokenizer.tokenize, ngram_range=(1, 3))\n",
    "    vectorizer.fit(train_df['free_text'])\n",
    "\n",
    "    x_train = vectorizer.transform(train_df['free_text'])\n",
    "    x_valid = vectorizer.transform(valid_df['free_text'])\n",
    "\n",
    "    y_train = train_df['label_id']\n",
    "    y_valid = valid_df['label_id']\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # threshold currently 0.5\n",
    "    preds = model.predict(x_valid)\n",
    "    print(f'Fold {fold_}')\n",
    "    print(accuracy_score(y_valid, preds))\n",
    "    print(classification_report(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite an improvement for only a little change.  \n",
    "\n",
    "\n",
    "Average accuracy : ~0.88\n",
    "\n",
    "Average F1 : ~0.93\n",
    "\n",
    "Average precision : ~0.91\n",
    "\n",
    "Average Recall : ~0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(\n",
    "            0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(\n",
    "            0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": loss\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    return model, optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, valid_loader, epochs=5, file_name=\"checkpoint.pth\"):\n",
    "    train_losses = []\n",
    "    valid_losses = [] \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "        for data in pbar:\n",
    "            x, y = data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({'Train Loss': train_loss / len(train_loader)})\n",
    "            train_losses.append(train_loss / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in valid_loader:\n",
    "                x, y = data\n",
    "                y_pred = model(x)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                valid_loss += loss.item()\n",
    "            valid_losses.append(valid_loss / len(valid_loader))\n",
    "        \n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss / len(train_loader)}, Valid Loss: {valid_loss / len(valid_loader)}')\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, loss, filename=file_name)\n",
    "\n",
    "        return train_losses, valid_losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 150\n",
    "pad_token = \"<pad>\"\n",
    "unk_token = \"<unk>\"\n",
    "def pad_tokens(tokens):\n",
    "    if (len(tokens) >= MAX_LENGTH):\n",
    "        return tokens[:MAX_LENGTH]\n",
    "    else:\n",
    "        return tokens + [pad_token] * (MAX_LENGTH - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c105ErrorC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES7_PKv\n  Referenced from: <5436ECC1-6F45-386E-B542-D5F76A22B52C> /Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <69A84A04-EB16-3227-9FED-383D2FE98E93> /Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torch/lib/libc10.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTokenizer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_tokens\u001b[39m(df_series):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df_series:\n",
      "File \u001b[0;32m~/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torch/_ops.py:1295\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1290\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1295\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c105ErrorC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES7_PKv\n  Referenced from: <5436ECC1-6F45-386E-B542-D5F76A22B52C> /Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <69A84A04-EB16-3227-9FED-383D2FE98E93> /Users/trungpham/Public/Real-time-Hate-speech-detection/.venv/lib/python3.11/site-packages/torch/lib/libc10.dylib"
     ]
    }
   ],
   "source": [
    "from pyvi import ViTokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "def yield_tokens(df_series):\n",
    "    for text in df_series:\n",
    "        yield ViTokenizer.tokenize(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df_train['free_text']), specials=[pad_token, unk_token])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train vs valid loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['free_text']\n",
    "        padded_tokens = pad_tokens(ViTokenizer.tokenize(text))\n",
    "        ids = torch.tensor(vocab.lookup_indices(padded_tokens))\n",
    "        y = row['label_id']\n",
    "        return ids, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m TextDataset(df_train, \u001b[43mvocab\u001b[49m)\n\u001b[1;32m      2\u001b[0m valid_ds \u001b[38;5;241m=\u001b[39m TextDataset(df_valid, vocab)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = TextDataset(df_train, vocab)\n",
    "valid_ds = TextDataset(df_valid, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                      shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE*2,\n",
    "                    num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size=1, hidden_size=128, num_layers=2, num_classes=1)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, optimizer, loss_fn, train_dl, val_dl, epochs=5, file_name=\"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
