{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1373314,"sourceType":"datasetVersion","datasetId":798371},{"sourceId":9150795,"sourceType":"datasetVersion","datasetId":5527576}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch \nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/vishd-comments-dataset/train.csv')\ndf_test = pd.read_csv('/kaggle/input/vishd-comments-dataset/test.csv')\ndf_valid = pd.read_csv('/kaggle/input/vishd-comments-dataset/dev.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data cleaning","metadata":{}},{"cell_type":"markdown","source":"### Remove na values","metadata":{}},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[df_train['free_text'].isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Just drop it \ndf_train = df_train.dropna(subset=['free_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clean out the emoji","metadata":{}},{"cell_type":"code","source":"import re\n\ndf_train['free_text'] = df_train['free_text'].apply(lambda x: re.sub(r'[^\\w\\s#@/:%.,_-]', '', x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['free_text'].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Standardize the vietnamese text","metadata":{}},{"cell_type":"code","source":"# lowercase all \ndf_train['free_text'] = df_train['free_text'].apply(lambda x: x.lower())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separate punctuation from words\ndf_train['free_text'] = df_train['free_text'].apply(lambda x: re.sub(r'(?<=[^\\s])\\s*([^\\w\\s])', r' \\1', x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I choose not to remove punctuations in this case as it may represent a sentence structure that as a whole shapes an offensive or not sentence. Thus removing punctuation may disrupt the natural structure of the text and impact downstream classifying tasks","metadata":{}},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # optional, turn bad worlds into its original form\n# # form the bad words dictionaries\n# bad_words_txt = '../vn_offensive_words.txt'\n# bad_words_dict = {}\n# with open(bad_words_txt, 'r') as f:\n#     bad_words = f.read().splitlines()\n#     origin = \"\"\n\n#     for sent in bad_words:\n#         temp = sent.split(' ')\n\n#         if (len(temp) > 1 and temp[0] == '#'):\n#             origin = ' '.join(temp[1:])\n#             continue\n        \n#         if (origin != \"\"):\n#             bad_words_dict[sent] = origin","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # sorry for the bad words :(\n# bad_words_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # replace all bad words variants with its original form\n# def replace_bad_words(text):\n#     for bad, origin in bad_words_dict.items():\n#         text = text.replace(bad, origin)\n#     return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function might be useful later","metadata":{}},{"cell_type":"markdown","source":"### Check output distribution","metadata":{}},{"cell_type":"code","source":"df_train['label_id'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have the following label:\n*   0: non-offensive\n*   1: Offensive\n*   2: Hate \n\nWe see here the data is imbalance.\n1 and 2 are similar, differ only at its level of hate. Thus as 0s outnumber the other 2 labels, we shall merge 1 and 2","metadata":{}},{"cell_type":"code","source":"df_train['label_id'] = df_train['label_id'].apply(lambda x: 1 if x in [1, 2] else x)\ndf_train['label_id'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"yet the data is still imbalance. If we predict 0 for all case we would have 0.82 accuracy!We shall counter it with the choice of metrics later","metadata":{}},{"cell_type":"markdown","source":"### Apply the same processing step for test and valid","metadata":{}},{"cell_type":"code","source":"# drop na\ndf_test = df_test.dropna(subset=['free_text'])\ndf_valid = df_valid.dropna(subset=['free_text'])\n\n# clean the emoji \ndf_valid['free_text'] = df_valid['free_text'].apply(lambda x: re.sub(r'[^\\w\\s#@/:%.,_-]', '', x))\ndf_test['free_text'] = df_test['free_text'].apply(lambda x: re.sub(r'[^\\w\\s#@/:%.,_-]', '', x))\n\n# standardize the text\n# lowercase all\ndf_valid['free_text'] = df_valid['free_text'].apply(lambda x: x.lower())\ndf_test['free_text'] = df_test['free_text'].apply(lambda x: x.lower())\n\n# separate punctuation from words\ndf_valid['free_text'] = df_valid['free_text'].apply(lambda x: re.sub(r'(?<=[^\\s])\\s*([^\\w\\s])', r' \\1', x))\ndf_test['free_text'] = df_test['free_text'].apply(lambda x: re.sub(r'(?<=[^\\s])\\s*([^\\w\\s])', r' \\1', x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the 1 and 2 labels\ndf_valid['label_id'] = df_valid['label_id'].apply(lambda x: 1 if x in [1, 2] else x)\ndf_test['label_id'] = df_test['label_id'].apply(lambda x: 1 if x in [1, 2] else x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid['label_id'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"### Benchmark : Bag-of-words with logistic regression","metadata":{}},{"cell_type":"markdown","source":"We use this basic model as a simple benchmark for out task","metadata":{}},{"cell_type":"code","source":"!pip install pyvi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply k fold\nfrom sklearn.model_selection import StratifiedKFold\n\ndf_train['kfold'] = -1\n\ndf_train = df_train.sample(frac=1).reset_index(drop=True)\n\ny = df_train['label_id'].values\n\nkf = StratifiedKFold(n_splits=5)\nfor f, (t_, v_) in enumerate(kf.split(X=df_train, y=y)):\n    df_train.loc[v_, 'kfold'] = f\n\ndf_train['kfold'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom pyvi import ViTokenizer\n\nfor fold_ in range(5):\n    train_df = df_train[df_train.kfold != fold_]\n    valid_df = df_train[df_train.kfold == fold_]\n    \n    vectorizer = CountVectorizer(tokenizer=ViTokenizer.tokenize)\n    vectorizer.fit(train_df['free_text'])\n    \n    x_train = vectorizer.transform(train_df['free_text'])\n    x_valid = vectorizer.transform(valid_df['free_text'])\n\n    y_train = train_df['label_id']\n    y_valid = valid_df['label_id']\n    \n    model = LogisticRegression()\n    model.fit(x_train, y_train)\n\n    # threshold currently 0.5\n    preds = model.predict(x_valid)\n    print(f'Fold {fold_}')\n    print(accuracy_score(y_valid, preds))\n    print(classification_report(y_valid, preds))","metadata":{"execution":{"iopub.status.busy":"2024-08-16T03:43:33.373624Z","iopub.execute_input":"2024-08-16T03:43:33.374437Z","iopub.status.idle":"2024-08-16T03:44:45.995659Z","shell.execute_reply.started":"2024-08-16T03:43:33.374387Z","shell.execute_reply":"2024-08-16T03:44:45.994155Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 0\n0.8318087318087318\n              precision    recall  f1-score   support\n\n           0       0.84      0.98      0.91      3977\n           1       0.56      0.15      0.23       833\n\n    accuracy                           0.83      4810\n   macro avg       0.70      0.56      0.57      4810\nweighted avg       0.79      0.83      0.79      4810\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 1\n0.835308796007486\n              precision    recall  f1-score   support\n\n           0       0.85      0.98      0.91      3977\n           1       0.60      0.15      0.24       832\n\n    accuracy                           0.84      4809\n   macro avg       0.72      0.56      0.57      4809\nweighted avg       0.80      0.84      0.79      4809\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 2\n0.8405073819920982\n              precision    recall  f1-score   support\n\n           0       0.85      0.98      0.91      3977\n           1       0.65      0.17      0.27       832\n\n    accuracy                           0.84      4809\n   macro avg       0.75      0.58      0.59      4809\nweighted avg       0.81      0.84      0.80      4809\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fold 3\n0.8369723435225619\n              precision    recall  f1-score   support\n\n           0       0.85      0.98      0.91      3977\n           1       0.63      0.14      0.23       832\n\n    accuracy                           0.84      4809\n   macro avg       0.74      0.56      0.57      4809\nweighted avg       0.81      0.84      0.79      4809\n\nFold 4\n0.8351008525681015\n              precision    recall  f1-score   support\n\n           0       0.85      0.97      0.91      3977\n           1       0.58      0.17      0.26       832\n\n    accuracy                           0.84      4809\n   macro avg       0.71      0.57      0.58      4809\nweighted avg       0.80      0.84      0.80      4809\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Say in this problem, I think we focus on recall better, as the cost of missing some hateful comments is high (may affect children). Our recall is already very high.\n\nAverage accuracy : ~0.84\n\nAverage F1 : ~0.91\n\nAverage precision : ~0.85\n\nAverage Recall : ~0.97","metadata":{}},{"cell_type":"markdown","source":"**Let's now test the performance on the valid set**","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer=ViTokenizer.tokenize)\nvectorizer.fit(df_train['free_text'])\n\nx_train = vectorizer.transform(df_train['free_text'])\nx_valid = vectorizer.transform(df_valid['free_text'])\n\ny_train = df_train['label_id']\ny_valid = df_valid['label_id']\n\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\n\n# threshold currently 0.5\npreds = model.predict(x_valid)\nprint(accuracy_score(y_valid, preds))\nprint(classification_report(y_valid, preds))","metadata":{"execution":{"iopub.status.busy":"2024-08-16T03:44:45.997490Z","iopub.execute_input":"2024-08-16T03:44:45.997892Z","iopub.status.idle":"2024-08-16T03:45:02.882612Z","shell.execute_reply.started":"2024-08-16T03:44:45.997859Z","shell.execute_reply":"2024-08-16T03:45:02.881136Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"0.8297155688622755\n              precision    recall  f1-score   support\n\n           0       0.84      0.98      0.90      2190\n           1       0.60      0.16      0.26       482\n\n    accuracy                           0.83      2672\n   macro avg       0.72      0.57      0.58      2672\nweighted avg       0.80      0.83      0.79      2672\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Use n-gram\n","metadata":{}},{"cell_type":"markdown","source":"Usually bad word phrases in Vietnamese goes in pair or group of 3, I have an intuition that using n-gram with bag-of-words can be useful. \n\nIt is also easy to implement","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom pyvi import ViTokenizer\n\nfor fold_ in range(5):\n    train_df = df_train[df_train.kfold != fold_]\n    valid_df = df_train[df_train.kfold == fold_]\n\n    vectorizer = CountVectorizer(tokenizer=ViTokenizer.tokenize, ngram_range=(1, 3))\n    vectorizer.fit(train_df['free_text'])\n\n    x_train = vectorizer.transform(train_df['free_text'])\n    x_valid = vectorizer.transform(valid_df['free_text'])\n\n    y_train = train_df['label_id']\n    y_valid = valid_df['label_id']\n\n    model = LogisticRegression()\n    model.fit(x_train, y_train)\n\n    # threshold currently 0.5\n    preds = model.predict(x_valid)\n    print(f'Fold {fold_}')\n    print(accuracy_score(y_valid, preds))\n    print(classification_report(y_valid, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quite an improvement for only a little change.  \n\n\nAverage accuracy : ~0.88\n\nAverage F1 : ~0.93\n\nAverage precision : ~0.91\n\nAverage Recall : ~0.95","metadata":{}},{"cell_type":"markdown","source":"\n**Let's save the model**","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Save the vectorizer\nwith open('vectorizer.pkl', 'wb') as f:\n    pickle.dump(vectorizer, f)\n\nwith open ('model.pkl', 'wb') as f:\n    pickle.dump(model, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's see how it perform on unseen examples**","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer(tokenizer=ViTokenizer.tokenize,ngram_range=(1, 3))\nvectorizer.fit(df_train['free_text'])\n\nx_train = vectorizer.transform(df_train['free_text'])\nx_valid = vectorizer.transform(df_valid['free_text'])\n\ny_train = df_train['label_id']\ny_valid = df_valid['label_id']\n\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)\n\npreds = model.predict(x_valid)\nprint(accuracy_score(y_valid, preds))\nprint(classification_report(y_valid, preds))","metadata":{"execution":{"iopub.status.busy":"2024-08-12T14:59:22.473905Z","iopub.execute_input":"2024-08-12T14:59:22.474402Z","iopub.status.idle":"2024-08-12T14:59:46.875705Z","shell.execute_reply.started":"2024-08-12T14:59:22.474364Z","shell.execute_reply":"2024-08-12T14:59:46.874485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quite impressive already. Perhaps this is due to, most of the comments that are labeled offensive or hate speech, are actually based on certain bad word phrases.","metadata":{}},{"cell_type":"markdown","source":"### Benchmark: Stacked-LSTM","metadata":{}},{"cell_type":"markdown","source":"Next let's try a deep model. See how it performs on the same dataset.","metadata":{}},{"cell_type":"markdown","source":"**Cuda check!!!**","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:41:34.123880Z","iopub.execute_input":"2024-08-13T01:41:34.124284Z","iopub.status.idle":"2024-08-13T01:41:34.163993Z","shell.execute_reply.started":"2024-08-13T01:41:34.124255Z","shell.execute_reply":"2024-08-13T01:41:34.162861Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### init model","metadata":{}},{"cell_type":"code","source":"import torch\n\n\nclass LSTM(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, emb_matrix=None):\n        super(LSTM, self).__init__()\n        self.emb = torch.nn.Embedding(num_embeddings= len(vocab), embedding_dim= input_size,\n                                      padding_idx=1, _weight=emb_matrix)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = torch.nn.LSTM(\n            input_size, hidden_size, num_layers, batch_first=True,dropout=0.1)\n        self.fc = torch.nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(\n            0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(\n            0), self.hidden_size).to(x.device)\n        \n        emb = self.emb(x)\n        out, _ = self.lstm(emb, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-08-13T03:04:01.387067Z","iopub.execute_input":"2024-08-13T03:04:01.388165Z","iopub.status.idle":"2024-08-13T03:04:01.403852Z","shell.execute_reply.started":"2024-08-13T03:04:01.388116Z","shell.execute_reply":"2024-08-13T03:04:01.402451Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"#### Utils for training","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, loss, filename=\"checkpoint.pth\"):\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"loss\": loss\n    }\n    torch.save(checkpoint, filename)\n\ndef load_checkpoint(model, optimizer, filename=\"checkpoint.pth\"):\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    epoch = checkpoint[\"epoch\"]\n    loss = checkpoint[\"loss\"]\n    return model, optimizer, epoch, loss","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:41:37.285807Z","iopub.execute_input":"2024-08-13T01:41:37.286229Z","iopub.status.idle":"2024-08-13T01:41:37.293504Z","shell.execute_reply.started":"2024-08-13T01:41:37.286195Z","shell.execute_reply":"2024-08-13T01:41:37.292412Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, loss_fn, train_loader, valid_loader, epochs=5, file_name=\"checkpoint.pth\"):\n    train_losses = []\n    valid_losses = [] \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n        train_loss_batch =[]\n        for data in pbar:\n            x, y = data\n            \n            optimizer.zero_grad()\n            y_pred = model(x)\n            \n            loss = loss_fn(y_pred, y)\n            loss.backward()\n            optimizer.step()\n\n\n            train_loss += loss.item()\n            pbar.set_postfix({'Train Loss': train_loss / len(train_loader)})\n            train_loss_batch.append(train_loss / len(train_loader))\n        \n        train_losses.append(np.mean(train_loss_batch))\n        \n        model.eval()\n        valid_loss = 0.0\n        valid_loss_batch = []\n        with torch.no_grad():\n            for data in valid_loader:\n                x, y = data\n                y_pred = model(x)\n                loss = loss_fn(y_pred, y)\n                valid_loss += loss.item()\n            valid_loss_batch.append(valid_loss / len(valid_loader))\n        valid_losses.append(np.mean(valid_loss_batch))\n        \n        print(f'Epoch {epoch}, Train Loss: {train_losses[-1]}, Valid Loss: {valid_losses[-1]}')\n\n    save_checkpoint(model, optimizer, epoch, loss, filename=file_name)\n    return train_losses, valid_losses\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:41:38.418472Z","iopub.execute_input":"2024-08-13T01:41:38.418861Z","iopub.status.idle":"2024-08-13T01:41:38.430308Z","shell.execute_reply.started":"2024-08-13T01:41:38.418829Z","shell.execute_reply":"2024-08-13T01:41:38.429331Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Preprocess ","metadata":{}},{"cell_type":"code","source":"!pip install underthesea","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:41:39.942858Z","iopub.execute_input":"2024-08-13T01:41:39.943253Z","iopub.status.idle":"2024-08-13T01:41:56.762995Z","shell.execute_reply.started":"2024-08-13T01:41:39.943224Z","shell.execute_reply":"2024-08-13T01:41:56.761949Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Collecting underthesea\n  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\nCollecting python-crfsuite>=0.9.6 (from underthesea)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.32.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)\nCollecting underthesea-core==1.0.4 (from underthesea)\n  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.7.4)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)\nDownloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\nSuccessfully installed python-crfsuite-0.9.10 underthesea-6.8.4 underthesea-core-1.0.4\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LENGTH = 150\npad_token = \"<pad>\"\nunk_token = \"<unk>\"\ndef pad_tokens(tokens):\n    if (len(tokens) >= MAX_LENGTH):\n        return tokens[:MAX_LENGTH]\n    else:\n        return tokens + [pad_token] * (MAX_LENGTH - len(tokens))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:41:56.765016Z","iopub.execute_input":"2024-08-13T01:41:56.765382Z","iopub.status.idle":"2024-08-13T01:41:56.773089Z","shell.execute_reply.started":"2024-08-13T01:41:56.765348Z","shell.execute_reply":"2024-08-13T01:41:56.772137Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from underthesea import word_tokenize\nfrom torchtext.vocab import build_vocab_from_iterator\n\n\ndef yield_tokens(df_series):\n    for text in df_series:\n        yield word_tokenize(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(df_train['free_text']), specials=[pad_token, unk_token])\n\nvocab.set_default_index(vocab[unk_token])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:41:56.774234Z","iopub.execute_input":"2024-08-13T01:41:56.774517Z","iopub.status.idle":"2024-08-13T01:42:20.971814Z","shell.execute_reply.started":"2024-08-13T01:41:56.774494Z","shell.execute_reply":"2024-08-13T01:42:20.970920Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### Create train vs valid loader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n\nclass TextDataset(Dataset):\n    def __init__(self, df, vocab):\n        self.df = df\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = row['free_text']\n        padded_tokens = pad_tokens(word_tokenize(text))\n        ids = torch.tensor(vocab.lookup_indices(padded_tokens))\n        y = row['label_id']\n        return ids, torch.tensor([y], dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:42:20.974110Z","iopub.execute_input":"2024-08-13T01:42:20.974632Z","iopub.status.idle":"2024-08-13T01:42:20.981875Z","shell.execute_reply.started":"2024-08-13T01:42:20.974604Z","shell.execute_reply":"2024-08-13T01:42:20.980938Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_ds = TextDataset(df_train, vocab)\nvalid_ds = TextDataset(df_valid, vocab)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:42:20.983753Z","iopub.execute_input":"2024-08-13T01:42:20.984287Z","iopub.status.idle":"2024-08-13T01:42:20.997501Z","shell.execute_reply.started":"2024-08-13T01:42:20.984230Z","shell.execute_reply":"2024-08-13T01:42:20.996435Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.dataloader import default_collate\n\nBATCH_SIZE = 8\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE,\n                      shuffle=True,\n                      collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\nval_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE*2,\n                    collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:42:20.998877Z","iopub.execute_input":"2024-08-13T01:42:20.999291Z","iopub.status.idle":"2024-08-13T01:42:21.010238Z","shell.execute_reply.started":"2024-08-13T01:42:20.999255Z","shell.execute_reply":"2024-08-13T01:42:21.009337Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"#### The training process","metadata":{}},{"cell_type":"code","source":"model = LSTM(input_size=128, hidden_size=256, num_layers=2, num_classes=1).to(device)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\ntrain_losses, valid_losses = train(model, optimizer, loss_fn, train_dl, val_dl, epochs=50, file_name=\"checkpoint.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:34:07.139678Z","iopub.execute_input":"2024-08-13T01:34:07.140512Z","iopub.status.idle":"2024-08-13T01:34:11.096245Z","shell.execute_reply.started":"2024-08-13T01:34:07.140481Z","shell.execute_reply":"2024-08-13T01:34:11.095185Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"Epoch 0:   5%|▍         | 146/3006 [00:03<01:15, 38.03it/s, Train Loss=0.0224]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[24], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, valid_loader, epochs, file_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)})\n\u001b[1;32m     22\u001b[0m train_loss_batch\u001b[38;5;241m.\u001b[39mappend(train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"#### graph out","metadata":{}},{"cell_type":"code","source":"import matplotlib.plt as plt\n\n# Create the plot\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')   \n\nplt.ylabel('Loss')\nplt.legend()\nplt.show()   ","metadata":{"execution":{"iopub.status.busy":"2024-08-12T15:44:13.004264Z","iopub.status.idle":"2024-08-12T15:44:13.004685Z","shell.execute_reply.started":"2024-08-12T15:44:13.004499Z","shell.execute_reply":"2024-08-12T15:44:13.004516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The valid loss is quite high.\n\nI believe this is  due to the large amount of unknown words in the valid sets not seen during training, and we are using fairly simple technique of embedding and thus not really prepare for unknown words.","metadata":{}},{"cell_type":"markdown","source":"#### Using FastText for better text representation","metadata":{}},{"cell_type":"markdown","source":"Based on the performance of the original lstm model, let's try using fasttext, which is a static text representation model that has support for Vietnamese,to see if it will improve the performance.","metadata":{}},{"cell_type":"code","source":"!pip install fasttext","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:42:21.011706Z","iopub.execute_input":"2024-08-13T01:42:21.012347Z","iopub.status.idle":"2024-08-13T01:42:34.544033Z","shell.execute_reply.started":"2024-08-13T01:42:21.012312Z","shell.execute_reply":"2024-08-13T01:42:34.542935Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Requirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (0.9.3)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext) (2.13.1)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext) (69.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fasttext) (1.26.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import fasttext \nft = fasttext.load_model('/kaggle/input/fasttext-vietnamese-word-vectors-full/cc.vi.300.bin')\nft.get_dimension()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T01:43:46.231214Z","iopub.execute_input":"2024-08-13T01:43:46.231518Z","iopub.status.idle":"2024-08-13T01:43:54.157547Z","shell.execute_reply.started":"2024-08-13T01:43:46.231494Z","shell.execute_reply":"2024-08-13T01:43:54.156458Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"300"},"metadata":{}}]},{"cell_type":"code","source":"vocab_size = len(vocab)\nprint(vocab_size)\nembedding_matrix = np.random.random((vocab_size, 300))\nembedding_vector = np.zeros(300)\nfor voc in tqdm(range(len(vocab))):\n    word = vocab.get_itos()[voc]\n    i = vocab.get_stoi()[word]\n    try:\n        embedding_vector = ft.get_word_vector(word)\n    except:\n        print(word, 'not found')\n    if embedding_vector is not None:\n        embedding_matrix[i, :] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2024-08-13T02:19:44.076804Z","iopub.execute_input":"2024-08-13T02:19:44.077784Z","iopub.status.idle":"2024-08-13T02:28:30.888612Z","shell.execute_reply.started":"2024-08-13T02:19:44.077743Z","shell.execute_reply":"2024-08-13T02:28:30.887559Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"24346\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 24346/24346 [08:46<00:00, 46.22it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding_matrix_tensor= torch.from_numpy(embedding_matrix).float()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T02:30:04.414683Z","iopub.execute_input":"2024-08-13T02:30:04.415372Z","iopub.status.idle":"2024-08-13T02:30:04.426214Z","shell.execute_reply.started":"2024-08-13T02:30:04.415335Z","shell.execute_reply":"2024-08-13T02:30:04.425215Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"model = LSTM(input_size=300, hidden_size=256, num_layers=2, num_classes=1, emb_matrix=embedding_matrix_tensor).to(device)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n\ntrain_losses, valid_losses = train(model, optimizer, loss_fn, train_dl, val_dl, epochs=50, file_name=\"checkpoint.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T02:19:09.529577Z","iopub.execute_input":"2024-08-13T02:19:09.530569Z","iopub.status.idle":"2024-08-13T02:19:09.536774Z","shell.execute_reply.started":"2024-08-13T02:19:09.530532Z","shell.execute_reply":"2024-08-13T02:19:09.535756Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"24346"},"metadata":{}}]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-13T02:19:26.792465Z","iopub.execute_input":"2024-08-13T02:19:26.793450Z","iopub.status.idle":"2024-08-13T02:19:26.799821Z","shell.execute_reply.started":"2024-08-13T02:19:26.793407Z","shell.execute_reply":"2024-08-13T02:19:26.798456Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"(24347, 300)"},"metadata":{}}]},{"cell_type":"markdown","source":"### PhoBert finetuning","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:09:50.429044Z","iopub.execute_input":"2024-08-12T09:09:50.429957Z","iopub.status.idle":"2024-08-12T09:09:50.434287Z","shell.execute_reply.started":"2024-08-12T09:09:50.429924Z","shell.execute_reply":"2024-08-12T09:09:50.433382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prepare data","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:09:51.538225Z","iopub.execute_input":"2024-08-12T09:09:51.538634Z","iopub.status.idle":"2024-08-12T09:10:04.641290Z","shell.execute_reply.started":"2024-08-12T09:09:51.538601Z","shell.execute_reply":"2024-08-12T09:10:04.640322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Model**","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModel, AutoConfig, XLMRobertaModel,\n    AutoTokenizer, AutoModelForSequenceClassification\n)\n\ninput_model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n\ninput_model.resize_token_embeddings(len(tokenizer))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:31:47.750864Z","iopub.execute_input":"2024-08-12T09:31:47.751573Z","iopub.status.idle":"2024-08-12T09:31:49.121653Z","shell.execute_reply.started":"2024-08-12T09:31:47.751537Z","shell.execute_reply":"2024-08-12T09:31:49.120696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for name, param in input_model.named_parameters():\n#     if 'classifier' not in name: # classifier layer\n#         param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:31:49.123691Z","iopub.execute_input":"2024-08-12T09:31:49.124146Z","iopub.status.idle":"2024-08-12T09:31:49.128026Z","shell.execute_reply.started":"2024-08-12T09:31:49.124112Z","shell.execute_reply":"2024-08-12T09:31:49.127191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have very little data, it is important that we only train the classifier head, or the last few layer of the model to prevent overfitting. thus in this step I freeze the whole bert-model and train the classifier layer only","metadata":{}},{"cell_type":"markdown","source":"#### Prepare dataset","metadata":{}},{"cell_type":"code","source":"def tokenize(my_str, tokenizer):\n    mapped_tokenize = tokenizer(my_str)\n\n    ids = mapped_tokenize['input_ids']\n    att_mask = mapped_tokenize['attention_mask']\n    return ids, att_mask","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:31:49.129296Z","iopub.execute_input":"2024-08-12T09:31:49.129705Z","iopub.status.idle":"2024-08-12T09:31:49.139730Z","shell.execute_reply.started":"2024-08-12T09:31:49.129674Z","shell.execute_reply":"2024-08-12T09:31:49.138937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass bert_dataset_from_df(Dataset):\n    def __init__(self, df, tokenizer,max_len=150):\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = row['free_text']\n        map_tokenized = self.tokenizer(text,padding='max_length',\n                                max_length = 64, truncation=True,\n                                return_tensors=\"pt\")\n        y = row['label_id']\n        target = torch.tensor([1,0], dtype=torch.float32)\n        if y == 0:\n            target = torch.tensor([1,0], dtype=torch.float32)\n        else:\n            target = torch.tensor([0,1], dtype=torch.float32)\n        return map_tokenized, target\n\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:31:49.141291Z","iopub.execute_input":"2024-08-12T09:31:49.141789Z","iopub.status.idle":"2024-08-12T09:31:49.151582Z","shell.execute_reply.started":"2024-08-12T09:31:49.141765Z","shell.execute_reply":"2024-08-12T09:31:49.150757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataloader\ntrain_ds = bert_dataset_from_df(df_train, tokenizer)\nval_ds = bert_dataset_from_df(df_valid,tokenizer)\n\ntrain_dl = DataLoader(train_ds,batch_size=8, shuffle=True)\nvalid_dl = DataLoader(val_ds,batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:31:49.634273Z","iopub.execute_input":"2024-08-12T09:31:49.635108Z","iopub.status.idle":"2024-08-12T09:31:49.639871Z","shell.execute_reply.started":"2024-08-12T09:31:49.635076Z","shell.execute_reply":"2024-08-12T09:31:49.638974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Utils for Bert model","metadata":{}},{"cell_type":"code","source":"def bert_train(model, train_dataloader, dev_dataloader, criterion_span, optimizer_spans, device, num_epochs):\n    train_losses = []\n    val_losses = []\n    model.to(device)\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        print('Epoch: ', epoch+1)\n        for texts, target in tqdm(train_dataloader):\n            input_ids = texts['input_ids'].squeeze(1).to(device)\n            attention_mask = texts['attention_mask'].to(device)\n            targets = target.to(device)\n\n            optimizer_spans.zero_grad()\n#             preds = model(input_ids, attention_mask)\n            preds = model(input_ids,token_type_ids=None,\n                          attention_mask=attention_mask, labels= targets)\n#             loss_span = criterion_span(preds, targets)\n            loss = preds.loss\n            loss.backward()\n\n            optimizer_spans.step()\n            total_loss += loss.item()\n            \n        \n        train_losses.append(total_loss/len(train_dataloader))\n\n        # Calculate validation loss and macro F1-score\n        val_loss = 0\n        for texts, target in tqdm(dev_dataloader):\n            input_ids = texts['input_ids'].squeeze(1).to(device)\n            attention_mask = texts['attention_mask'].to(device)\n            targets = target.to(device)\n            with torch.no_grad():\n                preds = model(input_ids,token_type_ids=None,\n                          attention_mask=attention_mask, labels= targets)\n                \n#                 loss_span = criterion_span(preds.squeeze(), targets)\n                loss = preds.loss\n                val_loss += loss #+ loss_label\n\n        val_losses.append(val_loss/len(dev_dataloader))\n    \n        print(f'Epoch {epoch}, Train Loss: {train_losses[-1]}, Valid Loss: {val_losses[-1]}')\n\n    save_checkpoint(model, optimizer_spans, epoch, train_losses, filename=\"bert_ckpt.pth\")\n    return train_losses, val_losses","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:31:50.734386Z","iopub.execute_input":"2024-08-12T09:31:50.734755Z","iopub.status.idle":"2024-08-12T09:31:50.746458Z","shell.execute_reply.started":"2024-08-12T09:31:50.734727Z","shell.execute_reply":"2024-08-12T09:31:50.745368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try training\nmodel = input_model\ncriterion_span = torch.nn.BCELoss()\noptimizer_spans = torch.optim.Adam(list(model.parameters()), lr=5e-6, weight_decay=1e-5)\nbert_train(model, train_dl, valid_dl, criterion_span, optimizer_spans, device, 5)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T09:31:51.223961Z","iopub.execute_input":"2024-08-12T09:31:51.224619Z","iopub.status.idle":"2024-08-12T09:52:07.069303Z","shell.execute_reply.started":"2024-08-12T09:31:51.224587Z","shell.execute_reply":"2024-08-12T09:52:07.068334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}